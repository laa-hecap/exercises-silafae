{
  "name": "ROOT + Ollama (same as CI)",
  "image": "rootproject/root:latest",
  "remoteUser": "root",

  "workspaceFolder": "/workspace",
  "mounts": [
    "source=${localWorkspaceFolder},target=/workspace,type=bind,consistency=cached"
  ],

  "forwardPorts": [11434],
  "containerEnv": {
    "OLLAMA_HOST": "http://127.0.0.1:11434"
  },

  "postCreateCommand": "bash -lc 'set -euxo pipefail; apt-get update; DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends curl git-lfs jq make g++ python3-pip; rm -rf /var/lib/apt/lists/*; git lfs install; python3 -m pip install --no-cache-dir numpy matplotlib uproot requests; curl -fsSL https://ollama.com/install.sh | sh'",

  "postStartCommand": "bash -lc 'set -euxo pipefail; pgrep -x ollama >/dev/null 2>&1 || (nohup ollama serve > /var/log/ollama.log 2>&1 & sleep 3); for m in llama3 mistral gemma2:2b deepseek-r1:1.5b; do curl -fsS -X POST http://127.0.0.1:11434/api/pull -d \"{\\\"name\\\":\\\"$m\\\"}\" || true; done'",

  "customizations": {
    "vscode": {
      "extensions": ["ms-python.python", "ms-toolsai.jupyter", "eamodio.gitlens"],
      "settings": { "python.defaultInterpreterPath": "/usr/bin/python3" }
    }
  }
}
